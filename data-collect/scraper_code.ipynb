{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T23:31:24.245573Z",
     "start_time": "2018-09-18T23:31:21.031817Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import random\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException, WebDriverException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up ChromeDriver for Selenium**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T23:31:31.611467Z",
     "start_time": "2018-09-18T23:31:31.605305Z"
    }
   },
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\"\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather data from sites with a DIV class and script HTML element \n",
    "* DIV class uses kib-grid item format\n",
    "* uses \\<script> with application/ld+json type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to collect links\n",
    "links = []\n",
    "\n",
    "# Collect links to collect data from\n",
    "url = f'<url of webpage with links to collect from>'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "tags = soup.find_all('div', attrs={'class':'kib-grid__item kib-grid__item--span-4@min-xs kib-grid__item--span-4@md kib-grid__item--span-4@min-lg'})\n",
    "\n",
    "# write collected links to file\n",
    "file = open(\"<filename to collect links in>.txt\",\"x\")\n",
    "\n",
    "for tag in tags:\n",
    "    link = tag.find('a')['href']\n",
    "    links.append(link)\n",
    "    \n",
    "    file.write(f'{link}\\n')\n",
    "\n",
    "file.close()\n",
    "driver.quit()\n",
    "print(f'Finished retrieving {len(links)} links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather actual data from collected links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-18T23:39:06.813101Z",
     "start_time": "2018-09-18T23:39:06.807476Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, since all the links obtained are part of an URL. Need to construct full URL.\n",
    "full_links = ['<input url without endpoint here>' + x for x in links]\n",
    "print(f'Check number of links: {len(full_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up scraper\n",
    "savepath = '<folder path to save scraped data>'\n",
    "links_count = 0  # to keep track of how many links the scraper has gone through\n",
    "\n",
    "for url in full_links:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(3) # add this so we don't need to wait for all webpage elements to load.\n",
    "    driver.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    data = [json.loads(x.string) for x in soup.find_all(\"script\", type=\"application/ld+json\")]\n",
    "    \n",
    "    filename = '_'.join(url.split('/')[-2:])\n",
    "    with open(f\"{savepath}/{filename}.json\", \"x\") as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "    links_count += 1\n",
    "    \n",
    "driver.quit()\n",
    "    \n",
    "print(f'Data collection is complete for {links_count} URLs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather data from sites with article format\n",
    "* DIV class in article format\n",
    "* uses \\<script> with application/ld+json type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to collect links\n",
    "links = []\n",
    "\n",
    "# Collect links to collect data from\n",
    "url = '<url of webpage with links to collect from>'\n",
    "\n",
    "# write collected links to file\n",
    "file = open(\"<filename to collect links in>.txt\",\"x\")\n",
    "\n",
    "for i in range(17):  # the number in range represents total pagination\n",
    "    if (i+1) == 1:\n",
    "        url_page = url\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url_page)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        tags = soup.find_all('div', attrs={'class':'article_card_articleCard__UmssU'})      \n",
    "        for tag in tags:\n",
    "            link = tag.find('a')['href']\n",
    "            links.append(link)\n",
    "            file.write(f'{link}\\n')\n",
    "            \n",
    "    else:\n",
    "        url_page = f'{url}/p/{i+1}#all-articles'\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url_page)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        tags = soup.find_all('div', attrs={'class':'article_card_articleCard__UmssU'})      \n",
    "        for tag in tags:\n",
    "            link = tag.find('a')['href']\n",
    "            links.append(link)\n",
    "            file.write(f'{link}\\n')\n",
    "\n",
    "file.close()\n",
    "driver.quit()\n",
    "print(f'Finished retrieving {len(links)} links')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather data from collected links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, since all the links obtained are part of an URL. Need to construct full URL.\n",
    "full_links = ['<input url without endpoint>' + x for x in links]\n",
    "print(f'Check number of links: {len(full_links)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up scraper\n",
    "savepath = '<folder path to save scraped data>'\n",
    "links_count = 0  # to keep track of how many links the scraper has gone through\n",
    "\n",
    "for url in full_links:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(3) # add this so we don't need to wait for all webpage elements to load.\n",
    "    driver.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    data = [json.loads(x.string) for x in soup.find_all(\"script\", type=\"application/ld+json\")]\n",
    "    \n",
    "    filename = '_'.join(url.split('/')[-2:])\n",
    "    with open(f\"{savepath}/{filename}.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile)\n",
    "        \n",
    "    links_count += 1\n",
    "    \n",
    "driver.quit()\n",
    "    \n",
    "print(f'Data collection is complete for {links_count} URLs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
