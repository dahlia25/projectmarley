{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236e01c8-179f-4528-b1ab-5cbd968ec0fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FLAN Fine Tuning Prototype\n",
    "By: Dahlia Ma\n",
    "\n",
    "This notebook is used to test the feasibility of fine-tuning the FLAN-T5 at different model sizes, with and without QLORA.\n",
    "\n",
    "Here is a reference to run LLMs using GPU on Mac: https://sebastianraschka.com/blog/2022/pytorch-m1-gpu.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba529fa9-1db9-4759-8042-f11fdb4cadd6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install & Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b796aba-c23e-4415-a53a-10cbca121a2b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes==0.41.2 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.41.2)\n",
      "Requirement already satisfied: transformers==4.35.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.35.0)\n",
      "Requirement already satisfied: peft==0.6.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.6.0)\n",
      "Requirement already satisfied: accelerate==0.25.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.25.0)\n",
      "Requirement already satisfied: einops==0.6.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: evaluate==0.4.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: scikit-learn==1.2.2 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.2.2)\n",
      "Requirement already satisfied: sentencepiece==0.1.99 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (0.1.99)\n",
      "Requirement already satisfied: wandb==0.15.3 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (0.15.3)\n",
      "Requirement already satisfied: pydantic==1.10.10 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (1.10.10)\n",
      "Requirement already satisfied: kaleido==0.2.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (0.2.1)\n",
      "Requirement already satisfied: cohere in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (4.37)\n",
      "Requirement already satisfied: openai in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (1.3.7)\n",
      "Requirement already satisfied: tiktoken==0.5.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions==4.5.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (4.5.0)\n",
      "Requirement already satisfied: gradio in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (3.50.2)\n",
      "Requirement already satisfied: tokenizers==0.14.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (0.14.1)\n",
      "Requirement already satisfied: trl in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.7.4)\n",
      "Requirement already satisfied: filelock in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from transformers==4.35.0->-r requirements.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: psutil in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from peft==0.6.0->-r requirements.txt (line 3)) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from peft==0.6.0->-r requirements.txt (line 3)) (2.1.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2.12.0)\n",
      "Requirement already satisfied: dill in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (2023.4.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from evaluate==0.4.0->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from scikit-learn==1.2.2->-r requirements.txt (line 7)) (2.2.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (8.0.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (3.1.40)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (1.38.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (68.0.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from wandb==0.15.3->-r requirements.txt (line 9)) (4.25.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from cohere->-r requirements.txt (line 12)) (3.8.5)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from cohere->-r requirements.txt (line 12)) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2.0,>=1.8 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from cohere->-r requirements.txt (line 12)) (1.9.0)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from cohere->-r requirements.txt (line 12)) (6.0.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from cohere->-r requirements.txt (line 12)) (1.26.16)\n",
      "Requirement already satisfied: anyio<4,>=3.5.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 13)) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 13)) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 13)) (0.25.2)\n",
      "Requirement already satisfied: sniffio in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from openai->-r requirements.txt (line 13)) (1.2.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (22.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (5.2.0)\n",
      "Requirement already satisfied: fastapi in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (0.103.2)\n",
      "Requirement already satisfied: ffmpy in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.6.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (0.6.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (2.1.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (3.7.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (9.4.0)\n",
      "Requirement already satisfied: pydub in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (2.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (0.24.0.post1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gradio->-r requirements.txt (line 16)) (11.0.3)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from trl->-r requirements.txt (line 18)) (0.6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 12)) (1.2.0)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 16)) (4.17.3)\n",
      "Requirement already satisfied: toolz in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 16)) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from anyio<4,>=3.5.0->openai->-r requirements.txt (line 13)) (3.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate==0.4.0->-r requirements.txt (line 6)) (11.0.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb==0.15.3->-r requirements.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 9)) (4.0.11)\n",
      "Requirement already satisfied: certifi in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 13)) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 13)) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 13)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from importlib_metadata<7.0,>=6.0->cohere->-r requirements.txt (line 12)) (3.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 16)) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 16)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 16)) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 16)) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 16)) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 16)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 6)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from pandas->evaluate==0.4.0->-r requirements.txt (line 6)) (2023.3)\n",
      "Requirement already satisfied: sympy in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.6.0->-r requirements.txt (line 3)) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.6.0->-r requirements.txt (line 3)) (3.1)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 18)) (0.15)\n",
      "Requirement already satisfied: rich>=11.1.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 18)) (13.7.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from tyro>=0.5.11->trl->-r requirements.txt (line 18)) (1.6.5)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from fastapi->gradio->-r requirements.txt (line 16)) (0.27.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.3->-r requirements.txt (line 9)) (5.0.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 16)) (0.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 18)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 18)) (2.15.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft==0.6.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/dahliama/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl->-r requirements.txt (line 18)) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "# install requirements (if needed)\n",
    "!pip install -r 'requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50bd2cf-70fa-4ef4-9d17-e9c036aa6d49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dahliama/anaconda3/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "Is CUDA available on torch? False\n",
      "Is MPS available on torch? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dahliama/anaconda3/lib/python3.11/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training\n",
    "    ,LoraConfig\n",
    "    ,get_peft_model\n",
    "    ,PeftModel\n",
    ")\n",
    "\n",
    "from peft.tuners.lora import LoraLayer\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM\n",
    "    ,AutoTokenizer\n",
    "    ,BitsAndBytesConfig\n",
    "    ,StoppingCriteria\n",
    "    ,StoppingCriteriaList\n",
    "    ,TrainingArguments\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# CUDA is not available on Mac, but 'mps' is analagous to CUDA\n",
    "torch.device(\"mps\")\n",
    "print(f'Is CUDA available on torch? {torch.cuda.is_available()}')\n",
    "print(f'Is MPS available on torch? {torch.backends.mps.is_available()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085785c5-2301-46dd-9b82-fc3e3aecb19f",
   "metadata": {},
   "source": [
    "## Set general variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2900e95b-cd2f-4c12-9d65-e6fa5a55e357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_type = \"mps\"  # set device to run model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba8e2e-97f2-4503-8bb6-44832af1dd67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load model & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c4d6bd-d20f-497a-843d-c1bef493603e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained model\n",
    "model_name = \"google/flan-t5-small\"\n",
    "\n",
    "''' 4-bit quantization requires more work to initiate; so won't use for prototype\n",
    "# config for 4-bit quantization for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True\n",
    "    ,bnb_4bit_use_double_quant=False\n",
    "    ,bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "'''\n",
    "\n",
    "# config for 8-bit quantization for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    # load_in_8bit = True  # CUDA not supported for Mac OS so cannot use this parameter\n",
    "    llm_int8_threshold = 6.0  # if outlier is above this threshold, then will run with lower fp16 precision\n",
    "    ,llm_int8_enable_fp32_cpu_offload = True\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name\n",
    "    # ,device_map = device_type\n",
    "    ,quantization_config=bnb_config\n",
    "    ,trust_remote_code=True\n",
    ")\n",
    "\n",
    "# set up and load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name \n",
    "    ,trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # set padding to the right to avoid issues with fp16 (when using 4-bit quantization)\n",
    "\n",
    "# check model parameters & structure\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dd5d8f-32b5-4fb7-9a33-ec7472269178",
   "metadata": {},
   "source": [
    "## Set PEFT parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2218f96-fd10-4b12-be20-55cedcd38e34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha = 16               # controls the scaling factor of loralib.linear weights\n",
    "    ,lora_dropout = 0.1           # dropout rate of loralib.linear\n",
    "    ,r = 64                       # dimension of low-rank matrix in lora adaptor\n",
    "    ,bias = \"none\"\n",
    "    ,task_type = \"SEQ_2_SEQ_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5003203-1dad-415c-a65a-d583e7e08940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add low rank adaptor to model\n",
    "model.add_adapter(peft_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee34e3-c51c-4ddf-9681-230540dc73a1",
   "metadata": {},
   "source": [
    "# Model Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "685004d5-d390-4aed-a182-eee2c8b974c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74572de2cc514c45b78e1166f532178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login() # log into HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d243869b-c19b-4b1c-a6e6-f9a5fca57ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mma-dahlia25\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=flan-t5-fine-tuning\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login() # log into wandb\n",
    "%env WANDB_PROJECT=flan-t5-fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62ca40-f1d8-43c2-9618-f46bc5c591a1",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41fc1e52-a266-4a22-b262-f3395877cc78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5e90221-eed7-4422-a2ea-dc4921199d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /Users/dahliama/.cache/huggingface/datasets/json/default-22288e17529495c0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b2de56e89a48b38e64b412dcd934fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dbc7a091f74c03afdf46608e677124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /Users/dahliama/.cache/huggingface/datasets/json/default-22288e17529495c0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd4efd4e7ba474fb4986ca965283112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 168\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files = 'trembling_qa_data.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f637ae63-9689-4d2d-b688-71dac8e2c546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the potential causes of a dog shivering or trembling?',\n",
       " 'answer': 'Causes may include joy, toxic food ingestion, pain, old age, nausea, poisoning, and more.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75170e17-9ab1-44d7-9f6f-db94cbfcabf6",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f4af5e79-6ee3-45ac-aa77-d17e7b3c6541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to format data to prompt instruction format\n",
    "def prompt_instruction_format(sample):\n",
    "    return f\"\"\"### Instruction:\n",
    "    You are a friendly and patient professional who cares about dogs. Use the given Input below to write the Response.\n",
    "    If you have not seen a similar input to Input, politely respond that it is not within your knowledge as a Response.\n",
    "\n",
    "    ### Input:\n",
    "    {sample['question']}\n",
    "\n",
    "    ### Response:\n",
    "    {sample['answer']}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "201f1bd3-35f4-442c-bc11-3893ab4badf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define training arguments to fine-tune model\n",
    "trainingArgs = TrainingArguments(\n",
    "    output_dir=model_name\n",
    "    ,num_train_epochs=10\n",
    "    ,per_device_train_batch_size=4  # batch size per GPU for training\n",
    "    ,gradient_accumulation_steps=2\n",
    "    ,gradient_checkpointing=True\n",
    "    ,optim=\"paged_adamw_32bit\"\n",
    "    ,logging_steps=3                # log onto console ever 'x' steps\n",
    "    ,save_strategy=\"epoch\"          # save after every epoch\n",
    "    ,learning_rate=2e-4\n",
    "    ,weight_decay=0.001\n",
    "    ,max_grad_norm=0.3\n",
    "    ,warmup_ratio=0.03\n",
    "    ,group_by_length=False\n",
    "    ,lr_scheduler_type=\"cosine\"\n",
    "    ,disable_tqdm=True\n",
    "    ,report_to=\"wandb\"\n",
    "    ,seed=55\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "832aadbf-b914-4366-8aa7-21da82483aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model\n",
    "    ,train_dataset=data['train']\n",
    "    ,peft_config=peft_config\n",
    "    ,max_seq_length=2048\n",
    "    ,tokenizer=tokenizer\n",
    "    ,packing=True\n",
    "    ,formatting_func=prompt_instruction_format\n",
    "    ,args=trainingArgs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cf256ee8-3617-4a07-b126-56cda97d8112",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'str2optimizer32bit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train/fine tune model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:280\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 280\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:1910\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1904\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   1905\u001b[0m             model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m   1906\u001b[0m             args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   1907\u001b[0m         )\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[0;32m-> 1910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m   1911\u001b[0m optimizer_was_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39moptimizer_step_was_skipped\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run:\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;66;03m# Delay optimizer scheduling until metrics are generated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/accelerate/optimizer.py:145\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accelerate_step_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:269\u001b[0m, in \u001b[0;36mOptimizer8bit.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state(group, p, gindex, pindex)\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefetch_state(p)\n\u001b[0;32m--> 269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_step(group, p, gindex, pindex)\n\u001b[1;32m    270\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_paged:\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# all paged operation are asynchronous, we need\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# to sync to make sure all tensors are in the right state\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bitsandbytes/optim/optimizer.py:469\u001b[0m, in \u001b[0;36mOptimizer2State.update_step\u001b[0;34m(self, group, p, gindex, pindex)\u001b[0m\n\u001b[1;32m    466\u001b[0m     gnorm_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat:\n\u001b[0;32m--> 469\u001b[0m     F\u001b[38;5;241m.\u001b[39moptimizer_update_32bit(\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_name,\n\u001b[1;32m    471\u001b[0m         grad,\n\u001b[1;32m    472\u001b[0m         p,\n\u001b[1;32m    473\u001b[0m         state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    474\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    475\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    476\u001b[0m         step,\n\u001b[1;32m    477\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    478\u001b[0m         state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    479\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    480\u001b[0m         config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    481\u001b[0m         gnorm_scale,\n\u001b[1;32m    482\u001b[0m         state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munorm_vec\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_unorm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    483\u001b[0m         max_unorm\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_unorm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    484\u001b[0m         skip_zeros\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_zeros\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    485\u001b[0m     )\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_wise\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    488\u001b[0m     F\u001b[38;5;241m.\u001b[39moptimizer_update_8bit(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_name,\n\u001b[1;32m    490\u001b[0m         grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m         max_unorm\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_unorm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    511\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bitsandbytes/functional.py:1206\u001b[0m, in \u001b[0;36moptimizer_update_32bit\u001b[0;34m(optimizer_name, g, p, state1, beta1, eps, step, lr, state2, beta2, weight_decay, gnorm_scale, unorm_vec, max_unorm, skip_zeros)\u001b[0m\n\u001b[1;32m   1204\u001b[0m optim_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m g\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[0;32m-> 1206\u001b[0m     optim_func \u001b[38;5;241m=\u001b[39m str2optimizer32bit[optimizer_name][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m g\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m   1208\u001b[0m     optim_func \u001b[38;5;241m=\u001b[39m str2optimizer32bit[optimizer_name][\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'str2optimizer32bit' is not defined"
     ]
    }
   ],
   "source": [
    "# Train/fine tune model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9bd28613-690c-4168-9731-3387e64fea48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0fbb21-0028-4306-9a4b-cf40a80fe0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070f0a8-bbf7-4f56-93e3-a0bb997855b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89609b01-8c7c-4551-9a2c-74a77d683156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ba4bc-3d69-4ee8-ba1d-b2c06e93d874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f7e37-2f37-4e53-8287-ddad81ba8989",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
